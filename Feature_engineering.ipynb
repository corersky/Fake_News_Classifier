{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from nltk import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Information\n",
    "\n",
    "Given the title of a fake news article A and the title of a coming news article B, participants are asked to classify B into one of the three categories.\n",
    "\n",
    "- agreed: B talks about the same fake news as A\n",
    "- disagreed: B refutes the fake news in A\n",
    "- unrelated: B is unrelated to A\n",
    "\n",
    "### Data fields\n",
    "\n",
    "- id - the id of each news pair.\n",
    "- tid1 - the id of fake news title 1.\n",
    "- tid2 - the id of news title 2.\n",
    "- title1_zh - the fake news title 1 in Chinese.\n",
    "- title2_zh - the news title 2 in Chinese.\n",
    "- title1_en - the fake news title 1 in English.\n",
    "- title2_en - the news title 2 in English.\n",
    "- label - indicates the relation between the news pair: agreed/disagreed/unrelated.\n",
    "- The English titles are machine translated from the related Chinese titles. This may help participants from all background to get better understanding of the datasets. Participants are highly recommended to use the Chinese version titles to finish the task.\n",
    "\n",
    "### File type\n",
    "\n",
    "- train.csv - training data contains 320,767 news pairs in both Chinese and English. This file provides the only data you can use to finish the task. Using external data is not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \n",
    "    data = pd.read_csv(path)\n",
    "    data = data.set_index(\"id\")\n",
    "    data = data.sort_index()\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to remove Punctuation from the string, even attached to the word. \n",
    "def RemovePunctuation(my_str):\n",
    "    punctuations = string.punctuation\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char.lower()\n",
    "            \n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the string\n",
    "def Tokenize(my_str):\n",
    "    text = my_str.split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Stop Word \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "def StopWordRemoval(my_str):\n",
    "    l = [word for word in my_str if word not in stopwords]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemma(my_str):\n",
    "    l = [lemmatizer.lemmatize(word) for word in my_str]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextProcessing(text):\n",
    "    \n",
    "    text = RemovePunctuation(text)\n",
    "    text = Tokenize(text)\n",
    "    text = StopWordRemoval(text)\n",
    "    text = lemma(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count and Length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_word_share(row):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), row['title1_en'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), row['title2_en'].split(\" \")))    \n",
    "    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordCount(data):\n",
    "    data['t1len'] = data['title1_en'].str.len()\n",
    "    data['t2len'] = data['title2_en'].str.len()\n",
    "\n",
    "    data['t1_n_words'] = data['title1_en'].apply(lambda row: len(row.split(\" \")))\n",
    "    data['t2_n_words'] = data['title2_en'].apply(lambda row: len(row.split(\" \")))\n",
    "    \n",
    "    data['word_share'] = data.apply(normalized_word_share, axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying the above define funcitons on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \"\"\"Creating another dataframe in to process the text \"\"\"\n",
    "    df = data[[\"title1_en\", \"title2_en\"]]\n",
    "    \n",
    "    \"\"\"Applying WordCount Feature\"\"\"\n",
    "    df = WordCount(df)\n",
    "    \n",
    "    \"\"\"Applying TextProcessing function\"\"\"\n",
    "    df[\"title1_en\"] = df[\"title1_en\"].apply(lambda x: TextProcessing(x))\n",
    "    df[\"title2_en\"] = df[\"title2_en\"].apply(lambda x: TextProcessing(x))\n",
    "    \n",
    "#     \"\"\"Creating Dictionary column by adding tile1_en and title2_en and applying Dictionary function to\"\"\"\n",
    "#     df[\"BagOfWords\"] = df['title1_en'] + df[\"title2_en\"]\n",
    "#     df.BagOfWords = df.BagOfWords.map(BagOfWords)\n",
    "    \n",
    "#     # Creating \"TF1\" and \"TF2\" variable containing a list of Frequency for each word in title1 and title 2 and\n",
    "#     # adding a column for Cosine Similarity. \n",
    "#     df[\"TF1\"] = TermFrequency(df.BagOfWords.values, df.title1_en.values)\n",
    "#     df[\"TF2\"] = TermFrequency(df.BagOfWords.values, df.title2_en.values)\n",
    "#     df[\"Cosine_similarity\"] = np.vectorize(Cosine)(df[\"TF1\"], df[\"TF2\"])\n",
    "#     df[\"senti\"] = data[[\"senti\"]]\n",
    "    df[\"label\"] = data[['label']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>t1len</th>\n",
       "      <th>t2len</th>\n",
       "      <th>t1_n_words</th>\n",
       "      <th>t2_n_words</th>\n",
       "      <th>word_share</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[two, new, oldage, insurance, benefit, old, pe...</td>\n",
       "      <td>[police, disprove, bird, nest, congress, perso...</td>\n",
       "      <td>94</td>\n",
       "      <td>111</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[come, shenzhen, sooner, later, son, also, com...</td>\n",
       "      <td>[gdp, overtopped, hong, kong, shenzhen, clarif...</td>\n",
       "      <td>144</td>\n",
       "      <td>73</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[come, shenzhen, sooner, later, son, also, com...</td>\n",
       "      <td>[shenzhens, gdp, topped, hong, kong, last, yea...</td>\n",
       "      <td>144</td>\n",
       "      <td>101</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[come, shenzhen, sooner, later, son, also, com...</td>\n",
       "      <td>[shenzhens, gdp, outstrips, hong, kong, shenzh...</td>\n",
       "      <td>144</td>\n",
       "      <td>106</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[come, shenzhen, sooner, later, son, also, com...</td>\n",
       "      <td>[shenzhens, gdp, overtakes, hong, kong, bureau...</td>\n",
       "      <td>144</td>\n",
       "      <td>107</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title1_en  \\\n",
       "id                                                      \n",
       "0   [two, new, oldage, insurance, benefit, old, pe...   \n",
       "1   [come, shenzhen, sooner, later, son, also, com...   \n",
       "2   [come, shenzhen, sooner, later, son, also, com...   \n",
       "3   [come, shenzhen, sooner, later, son, also, com...   \n",
       "4   [come, shenzhen, sooner, later, son, also, com...   \n",
       "\n",
       "                                            title2_en  t1len  t2len  \\\n",
       "id                                                                    \n",
       "0   [police, disprove, bird, nest, congress, perso...     94    111   \n",
       "1   [gdp, overtopped, hong, kong, shenzhen, clarif...    144     73   \n",
       "2   [shenzhens, gdp, topped, hong, kong, last, yea...    144    101   \n",
       "3   [shenzhens, gdp, outstrips, hong, kong, shenzh...    144    106   \n",
       "4   [shenzhens, gdp, overtakes, hong, kong, bureau...    144    107   \n",
       "\n",
       "    t1_n_words  t2_n_words  word_share      label  \n",
       "id                                                 \n",
       "0           17          18    0.057143  unrelated  \n",
       "1           28          11    0.078947  unrelated  \n",
       "2           28          15    0.071429  unrelated  \n",
       "3           28          15    0.071429  unrelated  \n",
       "4           28          16    0.046512  unrelated  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
